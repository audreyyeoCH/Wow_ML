# Neural Networks vs Gradient Boosting Method

I compared data generated by random walks of numerical features predicting machine failure (Torque, rotational speed, air temperature and pressure). I compared the Field's loss function feature in the Gradient Boosting Method with a simple and complex Neural Network with the raw data. Then I compared these methods with sampled raw data that create proportions of machine prediction "fail" : "no fail" ratio to be 50:50 (previously 30:70). I compared performance measures such as sensitivity, specificity, recall, precision, F1 and the ROC/AUC.

# spatial statistics education

When I started my MSci Biostatistics degree at the University of Zurich, in the odd semester in Spring 2018, the flavour of the Biostatistics Journal Club (run every semester or so) was Spatial statistics. I was petrified. Our prof lent us books I would never have felt an affinity to, until I learned about the wonderful possibilities that spatial statistics, in particular point level reference data, can be applied in. Namely, economics, public health and georgraphy. 

Please find my poster and presentation (cred to project buddy for moral support too) in situ. I felt like it was a success and the graphics very clear for a novice, about to start on a learning journey in spatial statistics !

# ml learning made accessible

If you think you had a hard time learning statistics and can't wrap your head around it. Here are more accesible material taken from recent publication on methods. That's right, I skipped the textbook and brought machine learning methods, to you. 

I did this not for application necessarily but it might be a friendly start (and I hope it is). If I can help you develop a sensitivity that methods are a complex world, and it is also difficult for those in it, then perhaps I have welcomed your visit into the scientific process. Iteration until Optimisation + with friendly collaboration. 

